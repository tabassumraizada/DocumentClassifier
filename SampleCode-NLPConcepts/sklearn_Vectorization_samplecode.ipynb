{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a sample code to demonstrate how Vectorization on text can be achieved using the sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=['NLP is an interesting area of work and NLP is getting popular', \n",
    "       'New algortithms are being build day by day', \n",
    "       'I am working on accuracy of classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the fit and transform components have been combined which can also be done in 2 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_fit=cv.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# This gives the number of unique words\n",
    "print(len(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, CountVectorizer does the following:\n",
    "\n",
    "1) Lowercases your text (set lowercase=false if you don’t want lowercasing)\n",
    "2) Uses utf-8 encoding\n",
    "3) Performs tokenization (converts raw text to smaller units of text)\n",
    "4) Uses word level tokenization (meaning each word is treated as a separate token)\n",
    "5) Ignores single characters during tokenization (say goodbye to words like ‘a’ and ‘I’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy', 'algortithms', 'am', 'an', 'and', 'are', 'area', 'being', 'build', 'by', 'classification', 'day', 'getting', 'interesting', 'is', 'new', 'nlp', 'of', 'on', 'popular', 'work', 'working']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scipy.sparse.csc_matrix.tocoo\n",
    "#csc_matrix.tocoo(copy=True)[source]\n",
    "#Return a COOrdinate representation of this matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cv_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 14,  3, 13,  6, 17, 20,  4, 12, 19])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_fit[0].tocoo().col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_fit[0].tocoo().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples=zip(cv_fit[0].tocoo().col,cv_fit[0].tocoo().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tuples=sorted(tuples, key=lambda x: (x[1],x[0]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 2),\n",
       " (14, 2),\n",
       " (20, 1),\n",
       " (19, 1),\n",
       " (17, 1),\n",
       " (13, 1),\n",
       " (12, 1),\n",
       " (6, 1),\n",
       " (4, 1),\n",
       " (3, 1)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nlp': 16,\n",
       " 'is': 14,\n",
       " 'an': 3,\n",
       " 'interesting': 13,\n",
       " 'area': 6,\n",
       " 'of': 17,\n",
       " 'work': 20,\n",
       " 'and': 4,\n",
       " 'getting': 12,\n",
       " 'popular': 19,\n",
       " 'new': 15,\n",
       " 'algortithms': 1,\n",
       " 'are': 5,\n",
       " 'being': 7,\n",
       " 'build': 8,\n",
       " 'day': 11,\n",
       " 'by': 9,\n",
       " 'am': 2,\n",
       " 'working': 21,\n",
       " 'on': 18,\n",
       " 'accuracy': 0,\n",
       " 'classification': 10}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show resulting vocabulary; the numbers are not counts, they are the position in the sparse vector.\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 1 0 0 0 0 0 1 1 2 0 2 1 0 1 1 0]\n",
      " [0 1 0 0 0 1 0 1 1 1 0 2 0 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#Words in the text are transformed to numbers and these numbers represent positional index in the sparse matrix as seen below\n",
    "print(cv_fit.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 22)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here 21 are the unique words in the input and 3 is the number of documents or texts that was used as the input\n",
    "cv_fit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will show some additional pre-processing steps that can be used by CountVectorizer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of custom stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(texts,stop_words=[\"of\",\"in\",\"the\",\"is\",\"an\",\"by\",\"are\"])\n",
    "cv_fit2=cv2.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of', 'in', 'the', 'is', 'an', 'by', 'are']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To check the stop words that are being used access cv.stop_words\n",
    "cv2.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv.stop_words_ (* with underscore suffix) gives you the stop words that CountVectorizer inferred from the settings:\n",
    "    min_df\n",
    "    max_df settings\n",
    "    those that were cut off during feature selection (through the use of max_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following settings consider document frequency. This can help elimiate words that might exist in 1-2 document with a high frequency\n",
    "\n",
    "1) min_df is a setting used to ignore words that have fewer occurrences than the number specified. These words are considered noise. This could be given as an absolute value (e.g. 1, 2, 3, 4) or a value representing proportion of documents (e.g 0.1 ignore words that have appeared in 10 % of the documents)\n",
    "\n",
    "2) max_df looks at how many documents contained a term, and if it exceeds the MAX_DF threshold, then it is eliminated from consideration. This could be given as an absolute value (e.g. 100, 200) or a value representing proportion of documents (e.g 0.85 ignore words that have appeared in 85 % of the documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore terms that appeared in less than 2 documents  and ignore words that have appeared in 85 % of the documents\n",
    "cv3 = CountVectorizer(texts,min_df=2,max_df=0.75)\n",
    "cv_fit3=cv3.fit_transform(texts)                                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy',\n",
       " 'algortithms',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'are',\n",
       " 'area',\n",
       " 'being',\n",
       " 'build',\n",
       " 'by',\n",
       " 'classification',\n",
       " 'day',\n",
       " 'getting',\n",
       " 'interesting',\n",
       " 'is',\n",
       " 'new',\n",
       " 'nlp',\n",
       " 'on',\n",
       " 'popular',\n",
       " 'work',\n",
       " 'working'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have to be careful with these setting because given the small number of documents we have this eliminated everything other than of :-)\n",
    "cv3.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'of': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv3.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other options to build on:\n",
    "ngram_range=(1,2),\n",
    "ngram_range=(2,2),analyzer='char_wb' # character level bi-gram, not sure where they are used though\n",
    "preprocessor=custom_preprocessor (define your own rules for pre-processing)\n",
    "max_features=1000 # limit features space b controlling vocabulary size\n",
    "binary=True # 0 and 1 values instead of counts, default values is False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References to learn other basics on this topic:\n",
    "\n",
    "Wiki resources:\n",
    "https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "\n",
    "sciki library links:\n",
    "\n",
    "Useful tutorials:\n",
    "\n",
    "10+ Examples for Using CountVectorizer. https://kavita-ganesan.com/how-to-use-countvectorizer/#.XzmAkuhKiUl\n",
    "How to Use Tfidftransformer & Tfidfvectorizer?. https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XzmFsehKiUk\n",
    "Gensim Word2Vec Tutorial – Full Working Example. https://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.XzmFvOhKiUk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
