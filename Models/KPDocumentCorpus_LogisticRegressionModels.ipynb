{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will classify the text in KP Document corpus into the 6 categories using Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C://Users//L833377//Desktop//ClassificationEngineIdeas//')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: In the first run we will use Word2vec for text pre-processing. Word2VEC converts in a text document into vectors where words with similar meaning have similar vector representation. It uses the surrounding words to represent target words with a Neural network that has a hidden layer and includes an encoding for word representation. We will first load the pre-trained Word2Vec library from Google which has been trained on billion word Google News Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Memorial_Hospital',\n",
       " 'Seniors',\n",
       " 'memorandum',\n",
       " 'elephant',\n",
       " 'Trump',\n",
       " 'Census',\n",
       " 'pilgrims',\n",
       " 'De',\n",
       " 'Dogs',\n",
       " '###-####_ext',\n",
       " 'chaotic',\n",
       " 'forgive',\n",
       " 'scholar',\n",
       " 'Lottery',\n",
       " 'decreasing',\n",
       " 'Supervisor',\n",
       " 'fundamentally',\n",
       " 'Fitness',\n",
       " 'abundance',\n",
       " 'Hold']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View some of the available vocabularies\n",
    "from itertools import islice\n",
    "list(islice(wv.vocab, 1030, 1045))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are using the bad of words approach \n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Label', 'Text', 'DocName', 'DocType', 'cleanText', 'Length']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df1, test_size=0.2, random_state = 42)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['LemmatizedcleanText']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['LemmatizedcleanText']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead). [ipykernel_launcher.py:8]\n"
     ]
    }
   ],
   "source": [
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Usage: \n",
    "class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)[source]\n",
    "\n",
    "n_jobsint, default=None\n",
    "Number of CPU cores used when parallelizing over classes if multi_class=’ovr’”. This parameter is ignored when the solver is set to ‘liblinear’ regardless of whether ‘multi_class’ is specified or not. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. \n",
    "\n",
    "Cfloat, default=1.0\n",
    "Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning. [logistic.py:432]\n",
      "FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning. [logistic.py:469]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(X_train_word_average, train['Label'])\n",
    "y_pred = logreg.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9310344827586207\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            Claims       1.00      1.00      1.00         4\n",
      "HealthCareDelivery       1.00      1.00      1.00         3\n",
      "        Membership       0.67      1.00      0.80         4\n",
      "          Pharmacy       1.00      1.00      1.00         4\n",
      "ProductandBenefits       1.00      0.80      0.89        10\n",
      "ProviderandNetwork       1.00      1.00      1.00         4\n",
      "\n",
      "          accuracy                           0.93        29\n",
      "         macro avg       0.94      0.97      0.95        29\n",
      "      weighted avg       0.95      0.93      0.93        29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, test.Label))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test.Label, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Next we will run the Logistic Regression model using TfIdfVectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.LemmatizedcleanText\n",
    "y = df1.Label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning. [logistic.py:432]\n",
      "FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning. [logistic.py:469]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=1, penalty='l2',\n",
       "                                    random_state=None, solver='warn',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8620689655172413\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            Claims       1.00      0.75      0.86         4\n",
      "HealthCareDelivery       1.00      1.00      1.00         3\n",
      "        Membership       0.80      1.00      0.89         4\n",
      "          Pharmacy       0.75      0.75      0.75         4\n",
      "ProductandBenefits       0.82      0.90      0.86        10\n",
      "ProviderandNetwork       1.00      0.75      0.86         4\n",
      "\n",
      "          accuracy                           0.86        29\n",
      "         macro avg       0.89      0.86      0.87        29\n",
      "      weighted avg       0.88      0.86      0.86        29\n",
      "\n",
      "Wall time: 39.7 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to approach 1 we notice that the test accuracy came down from 93 % to 86 %. The F1 score specially reduced for Pharmacy documents substantially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: We will now try to solve the classification problem using a Doc2vec vectorization approach. Doc2vec is based on Word2vec model, with the addition of another vector Doc ID to the input. Genisms Doc2vec requires each document to be have a label associated with it. This is done using TaggedDocument. \n",
    "\n",
    "Reference: https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df1.LemmatizedcleanText, df1.Label, random_state=0, test_size=0.2)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['cobi', 'claim', 'technical', 'debt', 'colorado', 'business', 'intelligence', 'cobi', 'claim', 'technical', 'debt', 'acquisition', 'layer', 'insert', 'update', 'historical', 'data', 'addressed', 'exception', 'eedm', 'delta', 'cdw', 'crds', 'exist', 'difficult', 'quantify', 'inconsistent', 'use', 'metadata', 'column', 'etl', 'follow', 'cobi', 'standard', 'conform', 'layerschema', 'provide', 'value', 'extra', 'layer', 'maintain', 'situation', 'background', 'claim', 'built', 'crds', 'time', 'cdw', 'went', 'live', 'predates', 'cobi', 'current', 'standard', 'cdw', 'utilize', 'traditional', 'type', 'logic', 'us', 'post', 'processing', 'track', 'update', 'much', 'crds', 'claim', 'design', 'done', 'without', 'data', 'etl', 'architect', 'oversight', 'cobi', 'claim', 'technical', 'debt', 'assessment', 'confirmed', 'delta', 'crds', 'cdw', 'net', 'delta', 'claim', 'line', 'time', 'delta', 'attribution', 'difficult', 'detect', 'quantify', 'extent', 'unknown', 'high', 'impact', 'example', 'change', 'internal', 'external', 'claim', 'identification', 'percentage', 'time', 'spent', 'rework', 'manual', 'fix', 'claim', 'data', 'significant', 'compared', 'subject', 'area', 'estimate', 'one', 'person', 'time', 'per', 'month', 'cobi', 'claim', 'technical', 'debt', 'assessment', '–', 'option', 'modify', 'acquisition', 'using', 'etl', 'pattern', 'remove', 'conform', 'layer', 'one', 'following', 'modify', 'etl', 'base', 'table', 'keep', 'base', 'model', 'remodel', 'base', 'cobibase', 'rewrite', 'eedm', 'apcd', 'etl', 'using', 'new', 'etl', 'pattern', 'change', 'eedm', 'data', 'model', 'report', 'remove', 'base', 'layer', 'repoint', 'eedm', 'apcd', 'acquisition', 'would', 'result', 'removal', 'crdsbaset', 'layer', 'remove', 'base', 'layer', 'create', 'new', 'dimensional', 'layer', 'claim', 'rebuild', 'eedm', 'apcd', 'using', 'new', 'dimensional', 'layer', 'cobi', 'claim', 'technical', 'debt', 'recommendation', 'modify', 'acquisition', 'using', 'etl', 'pattern', 'remove', 'conform', 'layer', 'modify', 'etl', 'base', 'table', 'keep', 'base', 'model', 'justification', 'recommendation', 'provides', 'benefit', 'roi', 'majority', 'challenge', 'occur', 'due', 'way', 'acquisition', 'base', 'layer', 'built', 'cobi', 'claim', 'technical', 'debt', 'issue', 'claim', 'built', 'crds', 'time', 'cdw', 'went', 'live', 'predates', 'cobi', 'current', 'standard', 'new', 'folder', 'cobiacqcdw', 'built', 'following', 'cobi', 'current', 'standard', 'etl', 'pattern', 'etl', 'classified', 'two', 'type', 'full', 'delta', 'incremental', 'depending', 'data', 'volume', 'table', 'width', 'full', 'delta', '–', 'record', 'le', 'field', 'incremental', '–', 'record', 'etl', 'pattern', 'full', 'delta', '–', 'acq', 'type', 'full', 'delta', 'in', 'incremental', '–', 'acq', 'type', 'incremental', 'in', 'acq', 'type', 'incremental', 'del', 'checklist', 'full', 'delta', 'etl', 'checklist', 'acq', 'type', 'full', 'delta', 'incremental', 'etl', 'checklist', 'acq', 'type', 'incremental', 'link', 'etl', 'pattern', 'new', 'acquisition', 'layer', 'issue', 'cdw', 'utilize', 'traditional', 'type', 'logic', 'us', 'post', 'processing', 'track', 'update', 'unlike', 'current', 'code', 'insert', 'etl', 'cobiacqcdw', 'built', 'following', 'acq', 'type', 'full', 'delta', '–', 'in', 'acq', 'type', 'incremental', 'in', 'acq', 'type', 'incremental', 'del', 'pattern', 'utilizes', 'logic', 'leaving', 'behind', 'process', 'post', 'processing', 'track', 'update', 'insert', 'update', 'deletes', 'altogether', 'taken', 'care', 'session', 'post', 'sql', 'property', 'plsql', 'procedure', 'cdc', 'comparing', 'sa', 'table', 'full', 'delta', 'table', 'incremental', 'table', 'qcare', 'table', 'copied', 'manually', 'etl', 'needed', 'data', 'load', 'one', 'time', 'table', 'scope', 'longer', 'used', 'data', 'production', 'new', 'acquisition', 'layer', 'issue', 'conform', 'layerschema', 'provide', 'value', 'extra', 'layer', 'maintain', 'current', 'design', 'new', 'design', 'conform', 'layer', 'exception', 'cocdwc', 'check', 'orphan', 'record', 'claim', 'header', 'record', 'found', 'xrefclmhdr', 'instnclmik', 'claim', 'header', 'record', 'found', 'xrefclmhdr', 'profclmik', 'claim', 'detail', 'record', 'found', 'xrefclmdtl', 'duplicate', 'check', 'instream', 'abort', 'duplicate', 'record', 'identified', 'conform', 'table', 'particular', 'run', 'xref', 'table', '–', 'abort', 'duplicate', 'record', 'identified', 'xref', 'table', 'compared', 'conform', 'table', 'parameterization', 'process', 'abort', 'exception', 'found', 'etlprocessexceptions', 'table', 'exception', 'clmhdr', 'clmdtl', 'invalid', 'authmbrmrnnb', 'new', 'code', 'cobiacqcdw', 'implementing', 'logic', 'cobi', 'pattern', 'following', 'cobi', 'standard', 'exception', 'processing', 'implementation', 'skipped', 'design', 'change', 'claim', 'technical', 'debt', 'introduced', 'four', 'new', 'table', 'base', 'layer', 'skbclaimheader', '–', 'implement', 'cdc', 'using', 'scraper', 'date', 'logic', 'truncatereload', 'every', 'run', 'instead', 'conform', 'layer', 'parameterization', 'process', 'every', 'etl', 'joined', 'lookup', 'apply', 'filter', 'limit', 'lookup', 'data', 'set', 'accordingly', 'reducing', 'lookup', 'cache', 'size', 'improves', 'performance', 'introduced', 'key', 'table', 'concept', 'per', 'cobi', 'standard', 'benefit', 'using', 'key', 'table', 'preserve', 'iks', 'anytime', 'child', 'table', 'truncated', 'reloading', 'maintain', 'key', 'throughout', 'join', 'table', 'instead', 'cfm', 'xref', 'table', 'kbclaimheader', '–', 'read', 'record', 'skbclaimheader', 'generates', 'base', 'header', 'iks', 'insert', 'etl', 'filtering', 'existing', 'claim', 'replacing', 'xref', 'table', 'concept', 'conform', 'layer', 'kbclaimdetail', '–', 'read', 'record', 'skbclaimheader', 'kbclaimheader', 'acdwinclmdtl', 'generates', 'base', 'detail', 'iks', 'insert', 'etl', 'filtering', 'existing', 'claim', 'replacing', 'xref', 'table', 'concept', 'conform', 'layer', 'sbclaimgl', '–', 'loading', 'gl', 'information', 'qcare', 'xcelys', 'claim', 'sbclaimgl', 'insert', 'truncatereload', 'table', 'replacing', 'current', 'sclmgl', 'table', 'sbclmgl', 'table', 'table', 'structure', 'sclmgl', 'except', 'one', 'field', 'bclaimdetailik', 'added', 'part', 'new', 'table', 'structure', 'therefore', 'made', 'sure', 'existing', 'process', 'table', 'structure', 'retains', 'without', 'change', 'benefit', 'using', 'bclaimdetailik', 'sbclmgl', 'whenever', 'join', 'table', 'limiting', 'join', 'condition', 'single', 'field', 'instead', 'multiple', 'join', 'condition', 'multiple', 'field', 'improves', 'performance', 'initial', 'load', 'reloading', 'data', 'nonprod', 'environment', 'multiple', 'time', 'sclmgl', 'table', 'gl', 'information', 'xcelys', 'qcare', 'claim', 'without', 'base', 'iks', 'linked', 'turn', 'go', 'multiple', 'join', 'condition', 'joining', 'table', 'might', 'run', 'performance', 'issue', 'benefit', 'introducing', 'sbclaimgl', 'table', 'instead', 'sclmgl', 'table', 'sclmglcurrent', 'sbclaimglnew', 'current', 'process', 'sclmgl', 'used', 'load', 'gl', 'information', 'reading', 'data', 'two', 'separate', 'pipeline', 'etl', 'cfm', 'cqcareinstnclmgl', 'xlskpglextrct', 'qcare', 'xcelys', 'claim', 'making', 'use', 'target', 'load', 'plan', 'etl', 'property', 'truncatereload', 'table', 'msclmglins', 'etl', 'two', 'session', 'created', 'loading', 'institutional', 'professional', 'claim', 'separately', 'appropriate', 'source', 'filter', 'overriding', 'sessionmapping', 'source', 'filter', 'property', 'new', 'table', 'sbclaimgl', 'read', 'data', 'three', 'gl', 'table', 'acdwxlglextrct', 'xcelys', 'professional', 'claim', 'acdwxlglextrct', 'xcelys', 'institutional', 'clmgl', 'qcare', 'three', 'source', 'filter', 'joined', 'respective', 'skbclaimheader', 'cdc', 'kbclaimdetail', 'linking', 'base', 'iks', 'source', 'sbclaimgl', 'acdwxlglextrct', 'xcelys', 'professional', 'claim', 'skbclaimheader', 'kbclaimdetail', 'acdwpfclmdtl', 'acdwxlglextrct', 'filter', 'skbclaimheadersrcsyscd', 'xcelys', 'roc', 'acdwpfclmdtlcrdsreccurr', 'acdwxlglextrctcrdsreccurr', 'acdwxlglextrctexpnstyptx', 'c', 'acdwxlglextrctcstcntrcd', 'acdwxlglextrctentcd', 'acdwxlglextrctasofenddt', 'yyyymmdd', 'acdwxlglextrct', 'xcelys', 'institutional', 'claim', 'skbclaimheader', 'kbclaimdetail', 'acdwinclmdtl', 'acdwxlglextrct', 'filter', 'skbclaimheadersrcsyscd', 'xcelys', 'roc', 'acdwinclmdtlcrdsreccurr', 'acdwxlglextrctcrdsreccurr', 'acdwxlglextrctexpnstyptx', 'c', 'acdwxlglextrctcstcntrcd', 'acdwxlglextrctentcd', 'acdwxlglextrctasofenddt', 'yyyymmdd', 'acdwxlglextrct', 'qcare', 'claim', 'skbclaimheader', 'kbclaimdetail', 'clmgl', 'filter', 'skbclaimheadersrcsyscd', '‘qcare', 'clmglglentcd', 'claim', 'header', 'design', 'cfm', 'without', 'cfm', 'cobase', 'current', 'code', 'read', 'data', 'three', 'conform', 'table', 'cinstnclmhdr', 'driver', 'table', 'cqcareinstnclmhdr', 'populating', 'field', 'incntwithldam', 'refclmcdref', 'cdshrtdstx', 'used', 'deriving', 'cfmmrktcd', 'etl', 'insert', 'new', 'code', 'skip', 'conform', 'table', 'read', 'directly', 'acquisition', 'layer', 'skbclaimheader', 'driver', 'table', 'cdc', 'acdwinclmhdr', 'actual', 'data', 'set', 'kbclaimheader', 'linking', 'base', 'iks', 'actual', 'data', 'set', 'instead', 'conform', 'xref', 'table', 'per', 'cobi', 'pattern', 'standard', 'compared', 'current', 'code', 'new', 'code', 'eliminates', 'one', 'source', 'another', 'converted', 'lookup', 'etl', 'cdshrtdstx', 'used', 'populate', 'cfmmrktcd', 'field', 'longer', 'used', 'stream', 'application', 'cqcareinstnclmhdr', 'converted', 'lookup', 'table', 'etl', 'claim', 'headercurrent', 'code', 'clmhdr', 'cinstnclmhdr', 'cqcareinstnclmhdr', 'refclmcdref', 'r', 'join', 'condition', 'cinstnclmhdrinstnclmik', 'cqcareinstnclmhdrinstnclmik', 'cinstnclmhdrrgncd', 'cqcareinstnclmhdrrgncd', 'cinstnclmhdrcrdsacqrownb', 'cqcareinstnclmhdrcrdsacqrownb', 'cinstnclmhdrprdctcd', 'refclmcdrefcdvltx', 'cinstnclmhdrsrcsyscd', 'refclmcdrefsrcsyscd', 'substrrefclmcdrefcdshrtdstx', 'co', 'pblsfc', 'sfp', 'sfdb', 'filter', 'cinstnclmhdrcrdsrcrdvld', 'cinstnclmhdrcrdsiudcd', 'qcfminstnclmik', 'qcfmrgncd', 'qcfmcrdsacqrownb', 'qcfmincntwithldam', 'rcdvltx', 'r', 'srcsyscd', 'rcdshrtdstx', 'used', 'deriving', 'cfmmrktcd', 'claim', 'headernew', 'code', 'skbclaimheader', 'acdwinclmhdr', 'acdwpfclmhdr', 'kbclaimheader', 'join', 'condition', 'instn', 'claim', 'skbclaimheaderclmhdrik', 'acdwinclmhdrinstnclmik', 'skbclaimheaderrgncd', 'acdwinclmhdrrgncd', 'skbclaimheadersrcsyscd', 'acdwinclmhdrsrcsyscd', 'skbclaimheaderclmsqncnb', 'acdwinclmhdrclmsqncnb', 'skbclaimheaderclmtypcd', '‘i’', 'skbclaimheaderclmhdrik', 'kbclaimheaderclmhdrik', 'skbclaimheaderrgncd', 'kbclaimheadersrcrgncd', 'skbclaimheadersrcsyscd', 'kbclaimheadersrcsrcsyscd', 'skbclaimheaderclmtypcd', 'kbclaimheadersrcclmtypcd', 'skbclaimheaderclmsqncnb', 'kbclaimheadersrcclmsqncnb', 'filter', 'cobiacqcdwacdwinclmhdrcrdsreccurr', 'bclmhdrik', 'lkpacdwqcinclmhdr', 'change', 'compared', 'cfm', 'clmhdrins', 'clmhdrupd', 'clmhdrdel', 'professional', 'claim', 'join', 'filter', 'condition', 'institutional', 'replacing', 'filter', 'pf', 'table', 'respectively', 'two', 'etl', 'separate', 'loading', 'professional', 'institutional', 'claim', 'clmhdr', 'table', 'claim', 'detail', 'current', 'design', 'clmdtl', 'read', 'data', 'three', 'source', 'qualifier', 'current', 'code', 'cinstnclmdtlqcare', 'left', 'join', 'sclmgl', 'qcare', 'cinstnclmdtlxcelys', 'left', 'join', 'sclmgl', 'xcelys', 'cinstnclmdtltapestry', 'left', 'join', 'bclmdtlgl', 'union', 'joined', 'brefsecondarypayorreason', 'join', 'cinstnclmdtlrsn', 'populating', 'secpayorcd', 'clmdtl', 'two', 'etl', 'insert', 'two', 'etl', 'updatesloading', 'instituitional', 'professional', 'claim', 'separately', 'cinstnclmdtlqcare', 'sclmglqcare', 'cinstnclmdtlxceley', 'sclmglxcelys', 'cinstnclmdtltapestr', 'bclmdtlgl', 'cinstnclmdtlrsn', 'brefsecondarypayorreason', 'cinstnclmdtlqcare', 'cinstnclmdtlxceley', 'cinstnclmdtltapestr', 'sqsecpayorcd', 'union', 'joiner', 'clmdtl', 'claim', 'detail', 'new', 'design', 'claim', 'detail', 'tapestry', '–', 'read', 'data', 'four', 'table', 'skbclaimheader', 'cdc', 'kbclaimdetail', 'bclaimdetailik', 'acdwinclmdtl', 'actual', 'data', 'set', 'bclmdtlgl', 'gl', 'information', 'tapestry', 'claim', 'load', 'clmdtl', 'using', 'two', 'etl', '–', 'one', 'institutional', 'claim', 'professional', 'claim', 'handle', 'insert', 'update', 'deletes', 'unlike', 'current', 'code', 'separate', 'etl', 'insert', 'update', 'brefsecondarypayorreason', 'join', 'cinstnclmdtlrsn', '–', 'converted', 'lookup', 'etl', 'unlike', 'current', 'code', 'table', 'source', 'claim', 'detail', 'xcelys', 'qcare', 'read', 'data', 'two', 'flow', 'institutional', 'skbclaimheader', 'cdc', 'kbclaimdetail', 'bclaimdetailik', 'acdwinclmdtl', 'actual', 'data', 'set', 'sbclmglinstitutional', 'gl', 'information', 'professional', 'skbclaimheader', 'cdc', 'kbclaimdetail', 'bclaimdetailik', 'acdwpfclmdtl', 'actual', 'data', 'set', 'sbclmglprofessional', 'gl', 'information', 'union', 'data', 'set', 'loaded', 'clmhdr', 'table', 'handle', 'insert', 'update', 'deletes', 'etl', 'unlike', 'current', 'code', 'claim', 'detail', 'tapestrynew', 'design', 'skbclaimheader', 'kbclaimdetail', 'acdwinclmdtl', 'bclmdtlglinstn', 'skbclaimheader', 'kbclaimdetail', 'acdwpfclmdtl', 'bclmdtlglprof', 'sqinstn', 'source', 'qualifier', 'clmhdrinstn', 'clmhdrprof', 'sqprof', 'source', 'qualifier', 'filter', 'instn', 'prof', 'etl', 'bclmdtlglmostrcntglverincdy', 'bclmdtlglgldeptcd', 'bclmdtlglglentcd', 'bclmdtlglgltrnscttypcd', 'claim', 'detail', 'qcare', 'xcelys', 'new', 'design', 'skbclaimheader', 'kbclaimdetail', 'acdwinclmdtl', 'sbclmglinstn', 'skbclaimheader', 'kbclaimdetail', 'acdwpfclmdtl', 'sbclmglprof', 'union', 'clmhdr', 'sqinstn', 'source', 'qualifier', 'sqprof', 'source', 'qualifier', 'simplify', 'optimize', 'parameterization', 'cdc', 'implemented', 'skbclaimheader', 'table', 'unlike', 'current', 'code', 'repeated', 'scraper', 'logic', 'every', 'etl', 'table', 'base', 'iks', 'base', 'ref', 'table', 'exact', 'copy', 'acquisition', 'converted', 'view', 'reducing', 'number', 'mapping', 'maintain', 'ancillary', 'table', 'inner', 'joined', 'skbclaimheader', 'cdc', 'kbclaimheader', 'kbclaimdetail', 'populating', 'base', 'iks', 'ever', 'possible', 'institutional', 'professional', 'claim', 'loaded', 'base', 'table', 'combined', 'single', 'etl', 'reduce', 'post', 'processing', 'avoid', 'aborted', 'job', 'base', 'table', 'table', 'handle', 'insert', 'update', 'deletes', 'avoids', 'majority', 'duplicate', 'abort', '“logic”', 'previously', 'handled', 'conform', 'layer', 'improvement', 'highlight', 'cfmcdw', 'cobiacqcdw', '\\uf0e0', 'crdsbaset', 'cdc', 'implemented', 'every', 'etl', 'conform', 'layer', 'using', 'lastextract', 'scraper', 'date', 'limited', 'one', 'skbclaimheader', 'base', 'table', 'using', 'skbclaimheaderprofextractfilterts', 'inst', 'prof', 'claim', 'one', 'etl', 'pulling', 'claim', 'directly', 'cobiacqcdw', 'table', 'skipping', 'cfm', 'layer', 'base', 'iks', 'base', 'iks', 'generated', 'conform', 'etl', 'loading', 'data', 'four', 'table', 'cinstnclmhdr', 'cprofclmhdr', 'cinstnclmdtl', 'cprofclmdtl', 'base', 'iks', 'generated', 'two', 'table', 'kbclaimheader', 'kbclaimdetail', 'table', 'skipping', 'conform', 'layer', 'orphan', 'record', 'xref', 'table', 'key', 'loaded', 'four', 'etl', 'mxrefclmhdrinsertnewinstnclaims', 'mxrefclmhdrinsertnewprofclaims', 'mxrefclmdtlinsertnewinstnclaims', 'mxrefclmdtlinsertnewprofclaims', 'table', 'joined', 'xref', 'table', 'invalidating', 'orphan', 'record', 'table', 'inner', 'joined', 'skbclaimheader', 'kbclaimheader', 'kbclaimdetail', 'two', 'thing', 'include', 'orphan', 'record', 'get', 'detail', 'information', 'header', 'information', 'cdc', 'implementation', 'avoiding', 'scraper', 'date', 'logic', 'every', 'single', 'etl', 'generic', 'point', 'individual', 'etl', 'institutional', 'professional', 'unnecessarily', 'increasing', 'number', 'etl', 'abort', 'etl', 'etlprocessexceptions', 'etl', 'handle', 'exception', 'postprocessing', 'four', 'xref', 'etl', 'load', 'instn', 'prof', '–', 'hdr', 'dtl', 'key', 'separate', 'etl', 'cfmcdw', 'even', 'table', 'base', 'iks', 'linked', 'one', 'etl', 'combining', 'instn', 'prof', 'wherever', 'possible', 'abort', 'etl', 'logic', 'handled', 'new', 'cobiacqcdw', 'table', 'two', 'table', 'kbclaimheader', 'kbclaimdetail', 'instn', 'prof', 'instead', 'four', 'xref', 'etl', 'view', 'created', 'top', 'cobiaacqcdw', 'table', 'base', 'iks', 'needed', 'link', 'detailed', 'document', 'parameterization', 'process', 'link', 'object', 'listcurrent', 'new', 'cobi', 'claim', 'technical', 'debt', 'acquisition', 'layer', 'insert', 'update', 'historical', 'data', 'addressed', 'exception', 'eedm', 'fixed', 'delta', 'cdw', 'crds', 'exist', 'difficult', 'quantify', 'fixed', 'inconsistent', 'use', 'metadata', 'column', 'fixed', 'etl', 'follow', 'cobi', 'standard', 'fixed', 'conform', 'layerschema', 'provide', 'value', 'extra', 'layer', 'maintain', 'fixed', 'issue', 'fixed', 'new', 'claim', 'design'], tags=['Train_0']),\n",
       " TaggedDocument(words=['membership', 'benefit', 'contract', 'overviewkpit', 'adhp', 'regional'], tags=['Train_1'])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:00<00:00, 283507.60it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:00<?, ?it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 283507.60it/s]\n",
      "100%|██████████| 141/141 [00:00<?, ?it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 283779.69it/s]\n",
      "100%|██████████| 141/141 [00:00<?, ?it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 313738.39it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 283779.69it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 284325.42it/s]\n",
      "100%|██████████| 141/141 [00:00<?, ?it/s]\n",
      "100%|██████████| 141/141 [00:00<?, ?it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 283779.69it/s]\n",
      "100%|██████████| 141/141 [00:00<?, ?it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 284052.29it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 284052.29it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 284325.42it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 284599.07it/s]\n",
      "100%|██████████| 141/141 [00:00<?, ?it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 282559.42it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 283779.69it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 283915.92it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 284052.29it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 304216.49it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 301425.52it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 304843.74it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 269920.98it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 284462.18it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 283915.92it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 284325.42it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 283779.69it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 299289.91it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning. [logistic.py:469]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='warn', n_jobs=1, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning. [logistic.py:432]\n",
      "FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning. [logistic.py:469]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5172413793103449\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            Claims       0.00      0.00      0.00         5\n",
      "HealthCareDelivery       0.75      0.60      0.67         5\n",
      "        Membership       0.30      0.60      0.40         5\n",
      "          Pharmacy       1.00      0.50      0.67         4\n",
      "ProductandBenefits       0.33      0.50      0.40         4\n",
      "ProviderandNetwork       0.83      0.83      0.83         6\n",
      "\n",
      "          accuracy                           0.52        29\n",
      "         macro avg       0.54      0.51      0.49        29\n",
      "      weighted avg       0.54      0.52      0.50        29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can see above that our accuracy substantially reduced with the Doc2Vec approach. This was expected because we have a small document corpus and Doc2vec works better for large corpus sizes with thousand of documents so that we can differentiate based on Document characteristics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
