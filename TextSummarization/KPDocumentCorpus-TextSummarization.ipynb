{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>DocName</th>\n",
       "      <th>DocType</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>Length</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Claims</td>\n",
       "      <td>Adjudication  Match Referrals to Claims  Certa...</td>\n",
       "      <td>Adjudication.docx</td>\n",
       "      <td>Word</td>\n",
       "      <td>adjudication match referrals claims certain se...</td>\n",
       "      <td>49095.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Claims</td>\n",
       "      <td>ALW_CHG_AMT at Claim Line Level =WDA_PRIMARY_A...</td>\n",
       "      <td>APCD Claims Calculation.docx</td>\n",
       "      <td>Word</td>\n",
       "      <td>alwchgamt claim line level wdaprimaryallowedam...</td>\n",
       "      <td>5481.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Claims</td>\n",
       "      <td>Region NW Mandated APCD in place? Yes Voluntar...</td>\n",
       "      <td>APCD Questions.pptx</td>\n",
       "      <td>powerpoint</td>\n",
       "      <td>region nw mandated apcd place yes voluntary ap...</td>\n",
       "      <td>4057.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Claims</td>\n",
       "      <td>APCD (All Payer Claims Database)  Overview  Th...</td>\n",
       "      <td>apcd-KPWA.docx</td>\n",
       "      <td>Word</td>\n",
       "      <td>apcd payer claims database overview washington...</td>\n",
       "      <td>2916.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Claims</td>\n",
       "      <td>Claims Data Warehouse\u000b",
       "CDW 101 Module\u000b",
       "How to Pi...</td>\n",
       "      <td>CDW 101 How to Pick Tables.pptx</td>\n",
       "      <td>powerpoint</td>\n",
       "      <td>claims data warehouse cdw module pick views de...</td>\n",
       "      <td>2468.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Label                                               Text  \\\n",
       "0  Claims  Adjudication  Match Referrals to Claims  Certa...   \n",
       "1  Claims  ALW_CHG_AMT at Claim Line Level =WDA_PRIMARY_A...   \n",
       "2  Claims  Region NW Mandated APCD in place? Yes Voluntar...   \n",
       "3  Claims  APCD (All Payer Claims Database)  Overview  Th...   \n",
       "4  Claims  Claims Data Warehouse\n",
       "CDW 101 Module\n",
       "How to Pi...   \n",
       "\n",
       "                           DocName     DocType  \\\n",
       "0                Adjudication.docx        Word   \n",
       "1     APCD Claims Calculation.docx        Word   \n",
       "2              APCD Questions.pptx  powerpoint   \n",
       "3                   apcd-KPWA.docx        Word   \n",
       "4  CDW 101 How to Pick Tables.pptx  powerpoint   \n",
       "\n",
       "                                           cleanText   Length  category_id  \n",
       "0  adjudication match referrals claims certain se...  49095.0            0  \n",
       "1  alwchgamt claim line level wdaprimaryallowedam...   5481.0            0  \n",
       "2  region nw mandated apcd place yes voluntary ap...   4057.0            0  \n",
       "3  apcd payer claims database overview washington...   2916.0            0  \n",
       "4  claims data warehouse cdw module pick views de...   2468.0            0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10000,sublinear_tf=True, min_df=6,norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "features = tfidf.fit_transform(df1.cleanText).toarray() # Remaps the words in the text column of \n",
    "                                                  # data frame into features (superset of words) with an importance assigned \n",
    "                                                  # based on each words frequency in the document and across documents\n",
    "\n",
    "labels = df1.category_id    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# We will use the entire dataset as it is small \n",
    "# Sampling a subset of our dataset because t-SNE is computationally expensive\n",
    "\n",
    "SAMPLE_SIZE = int(len(features))\n",
    "\n",
    "np.random.seed(0)\n",
    "indices = np.random.choice(range(len(features)), size=SAMPLE_SIZE, replace=False)         \n",
    "projected_features = TSNE(n_components=2, random_state=0).fit_transform(features[indices]) # Array of all projected features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for Word2Vec with Classification Model:https://github.com/susanli2016/NLP-with-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    adjud match referr claim certain servic might ...\n",
       "1    alwchgamt claim line level wdaprimaryallowedam...\n",
       "2    region nw mandat apcd place yes voluntari apcd...\n",
       "3    apcd payer claim databas overview washington s...\n",
       "4    claim data warehous cdw modul pick view decemb...\n",
       "Name: stemmedcleanText, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['stemmedcleanText'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summarization using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Claims Authorization Matching Logic (CAML) for Claims  Having CAML information for claims is very critical to several processes within Health Plan Operations. It will enable them to audit CAML rules more efficiently , prevent appointment leakage of members to outside providers and provide transaprency.  The three important CAML fields are;  Hierarchy ID - This is the product line (HMO, Medicare, PPOs, POS) and what attributes are required to match an authorization. Attributes are the claim and authorization to matc such as service date, provider number, place of service and are different with each hierarchy.  Priority Number - Within each hierarchy there are different priority levels. This number indicates that.  Criteria Number - Criteria within the priority levels and hierarchy establishes the criteria (rule) that the claim reads. The rule claims use to link to an authorization. It determines , if any, the additional components of a claim required to mach within hierarchy.  A new JUNK Dimension , DIM_CLAIM_AUTHORIZATION_RULE_MISC is created that will hold all the possible combinations of the above mentioned three fields.  DIM_CLAIM_AUTHORIZATION_RULE_MISC_KEY will be referenced in V_RPT_FACT_CLAIM_LINE and will have a value greater than zero whenever a given claim has CAML rules applied. Detailed information about the rules will be available in the dimension.    Details of the fields in this dimension are given below;  Column Name  Data Type  Column Description  DIM_CLAIM_AUTHORIZATION_RULE_MISC_KEY  integer  Generated artificial key  CRITERIA_NBR  integer  Criteria within the priority levels and hierarchy it gives us what criteria (rule) the claim reads. The rule claims use to link to an authorization. It determines whether what additional components of a claim are required to match within a hierarchy  CRITERIA_DESCR  varchar(1000)  The rule name claims used to determine  AUTHORIZATION_CODE  char(1)  Authorization Code (N = No auth required, M = Medical review required, P = Pre auth required for in-network coverage)  AUTHORIZATION_DESCR  varchar(200)  Authorization Description  HIERARCHY_NBR  integer  HHR is the product line (HMO, Medicare, PPOs,POS) and what attributes are required to match an authorization. Attributes are the requirements of the claim and auth to match such as service date, provider number, place of service and are different with each hierarchy  HIERARCHY_CODE  char(3)  Hierarchy Code  HIERARCHY_DESCR  varchar(1000)  Description  PRODUCT_DESCR  varchar(100)  Product associated with the Hierarchy code  PRIORITY_NBR  integer  Priority number within each hierarchy there are various priority levels. Identifies what order the CAML rules are processed in and the outcome  EDW_JOB_SEQUENCE_NBR  INTEGER  Job sequence number  EDW_INSERT_DTM  timestamp  Date and time the row was created in the data warehouse.  EDW_UPDATE_DTM  timestamp  Date and time the row was updated in the data warehouse.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Text'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Identifies what order the CAML rules are processed in and the outcome  EDW_JOB_SEQUENCE_NBR  INTEGER  Job sequence number  EDW_INSERT_DTM  timestamp  Date and time the row was created in the data warehouse.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import summarize from gensim\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization import keywords\n",
    "# convert text to string format\n",
    "text=str(df1['Text'][6])\n",
    "# Summarize the text with ratio 0.01 (1% of total words.)\n",
    "summarize(text,ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string \n",
    "import pandas as pd   \n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text2(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    #text = text.lower()\n",
    "    text = re.sub(r'\\[.?\\]', '.', text)\n",
    "    text = re.sub(r'[\\n\\r\\t]', '.', text)\n",
    "    #text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape('!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'), '', text)\n",
    "    \n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'[\\w\\.-]+@([\\w\\.-]+)','',text) # remove email addresses from text\n",
    "\n",
    "    # remove duplicates\n",
    "    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text) #remove duplicated words in row\n",
    "    text = text.replace('\\t', '.')\n",
    "    # Remove a sentence if it is only one word long\n",
    "    if len(text) > 2:\n",
    "        return ' '.join(word for word in text.split() if word not in STOPWORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Text2'] = pd.DataFrame(df1.Text.apply(lambda x: clean_text2(x)))\n",
    "#df1['Length']=df1['cleanText'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Treatment Plan  Plan a future visit\\t2  Document a treatment visit\\t3       Plan a future visit  Click , then click the location in Planned Treatments, Draft Treatments, or Today that you'd like to move it to.  Fill in any additional details for the treatment.  If you are planning a procedure to perform on multiple teeth, you can use commas or dashes in the Site field to efficiently enter multiple teeth at once. For example, if you're planning to do sealants on teeth 1-5, type 1-5 in the Site field for that procedure.  Click  Plan to send this information to a scheduler. The treatment moves from the Draft Treatments section to the Planned Treatments section.     If you know the treatment should be scheduled right away, you can add the findings directly to the Planned Treatments section instead. When finished, collapse the treatment to send the information to the scheduler.    Add treatment options  In the Draft Treatments section, click  Add Option or select an option template to create a different treatment plan and compare pricing options for a patient. To calculate an estimate for the patient, click Calculate Estimate at the top of the treatment.      To add more findings that the treatment plan addresses, click  next to the finding and add it to the treatment.     Adjust visits  Click  Visit to add a new visit to the current treatment plan.  You can move procedures between visits and treatment plans.  Click  to remove a visit.      To quickly reorganize your treatment plan, move procedures between visits or drag and drop visits to change their order.      Document a treatment visit  When the patient arrives for a scheduled treatment, open the visit and go to the Treatment Plan.  Move the visit to the Today section from either the Outstanding section or Planned Treatments section.   You can only move the visit, not the individual procedure or the entire treatment.  If you need to associate a billing diagnosis with your procedures, document one in the Visit Diagnoses section of the Treatment Plan Activity. Then click  to open the Treatment Plan - Billing Diagnoses window and associate the visit diagnosis with each procedure.  If the same diagnosis applies to all procedures in the visit, you can select the Associate the selected diagnosis with all procedures in this visit that have not been completed check box to save time.   After you've performed each procedure, click Completed.  When you've addressed the findings in the Addressed Findings section, click Resolved next to each finding.   If this is the last or only visit in the treatment, click  Complete Treatment Plan when you've finished all the procedures in the plan. If this isn't the last or only visit, skip this step.    Completed procedures appear in the Tooth Chart in blue.           © 2019 Epic Systems Corporation. All rights reserved. PROPRIETARY INFORMATION - This item and its contents may not be accessed, used, modified, reproduced, performed, displayed, distributed or disclosed unless and only to the extent expressly authorized by an agreement with Epic. This item is a Commercial Item, as that term is defined at 48 C.F.R. Sec. 2.101. It contains trade secrets and commercial information that are confidential, privileged and exempt from disclosure under the Freedom of Information Act and prohibited from disclosure under the Trade Secrets Act. After Visit Summary, Analyst, App Orchard, ASAP, Beaker, BedTime, Bones, Break-the-Glass, Caboodle, Cadence, Canto, Care Everywhere, Charge Router, Chronicles, Clarity, Cogito ergo sum, Cohort, Colleague, Community Connect, Cupid, Epic, EpicCare, EpicCare Link, Epicenter, Epic Earth, EpicLink, EpicWeb, Good Better Best, Grand Central, Haiku, Happy Together, Healthy Planet, Hyperspace, Kaleidoscope, Kit, Limerick, Lucy, MyChart, OpTime, OutReach, Patients Like Mine, Phoenix, Powered by Epic, Prelude, Radar, Resolute, Revenue Guardian, Rover, Share Everywhere, SmartForms, Sonnet, Stork, Tapestry, Trove, Welcome, Willow, Wisdom, and With the Patient at Heart are registered trademarks, trademarks or service marks of Epic Systems Corporation in the United States of America and/or other countries. Other company, product and service names referenced herein may be trademarks or service marks of their respective owners. U.S. and international patents issued and pending.    1            3\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Text'][26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If need associate billing diagnosis procedures document one Visit Diagnoses section Treatment Plan Activity.\\nThen click open Treatment Plan Billing Diagnoses window associate visit diagnosis procedure.\\nIf last visit treatment click Complete Treatment Plan youve finished procedures plan.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize the text with ratio 0.01 (1% of total words.)\n",
    "summarize(str(df1['Text2'][26]),ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# based on keywords it looks like we could have benefited from Stemming# based on keywords it looks like we could have benefited from Stemming\n",
    "\n",
    "print(keywords(text,ratio=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/Apress/natural-language-processing-recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "from sumy.summarizers.luhn import LuhnSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are planning a procedure to perform on multiple teeth, you can use commas or dashes in the Site field to efficiently enter multiple teeth at once.\n",
      "For example, if you're planning to do sealants on teeth 1-5, type 1-5 in the Site field for that procedure.\n",
      "When finished, collapse the treatment to send the information to the scheduler.\n",
      "You can only move the visit, not the individual procedure or the entire treatment.\n",
      "If you need to associate a billing diagnosis with your procedures, document one in the Visit Diagnoses section of the Treatment Plan Activity.\n",
      "Then click  to open the Treatment Plan - Billing Diagnoses window and associate the visit diagnosis with each procedure.\n",
      "If the same diagnosis applies to all procedures in the visit, you can select the Associate the selected diagnosis with all procedures in this visit that have not been completed check box to save time.\n",
      "PROPRIETARY INFORMATION - This item and its contents may not be accessed, used, modified, reproduced, performed, displayed, distributed or disclosed unless and only to the extent expressly authorized by an agreement with Epic.\n",
      "It contains trade secrets and commercial information that are confidential, privileged and exempt from disclosure under the Freedom of Information Act and prohibited from disclosure under the Trade Secrets Act.\n",
      "After Visit Summary, Analyst, App Orchard, ASAP, Beaker, BedTime, Bones, Break-the-Glass, Caboodle, Cadence, Canto, Care Everywhere, Charge Router, Chronicles, Clarity, Cogito ergo sum, Cohort, Colleague, Community Connect, Cupid, Epic, EpicCare, EpicCare Link, Epicenter, Epic Earth, EpicLink, EpicWeb, Good Better Best, Grand Central, Haiku, Happy Together, Healthy Planet, Hyperspace, Kaleidoscope, Kit, Limerick, Lucy, MyChart, OpTime, OutReach, Patients Like Mine, Phoenix, Powered by Epic, Prelude, Radar, Resolute, Revenue Guardian, Rover, Share Everywhere, SmartForms, Sonnet, Stork, Tapestry, Trove, Welcome, Willow, Wisdom, and With the Patient at Heart are registered trademarks, trademarks or service marks of Epic Systems Corporation in the United States of America and/or other countries.\n"
     ]
    }
   ],
   "source": [
    "# Install sumy\n",
    "\n",
    "#!pip install sumy\n",
    "\n",
    "# Import the packages\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "from sumy.summarizers.luhn import LuhnSummarizer \n",
    "\n",
    "# Extracting and summarizing\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 10\n",
    "parser = PlaintextParser.from_string(df1['Text'][26], Tokenizer(LANGUAGE))\n",
    "summarizer = LsaSummarizer()\n",
    "summarizer = LsaSummarizer(Stemmer(LANGUAGE))\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treatment Plan  Plan a future visit\t2  Document a treatment visit\t3       Plan a future visit  Click , then click the location in Planned Treatments, Draft Treatments, or Today that you'd like to move it to.\n",
      "The treatment moves from the Draft Treatments section to the Planned Treatments section.\n",
      "Add treatment options  In the Draft Treatments section, click  Add Option or select an option template to create a different treatment plan and compare pricing options for a patient.\n",
      "To add more findings that the treatment plan addresses, click  next to the finding and add it to the treatment.\n",
      "To quickly reorganize your treatment plan, move procedures between visits or drag and drop visits to change their order.\n",
      "Document a treatment visit  When the patient arrives for a scheduled treatment, open the visit and go to the Treatment Plan.\n",
      "If you need to associate a billing diagnosis with your procedures, document one in the Visit Diagnoses section of the Treatment Plan Activity.\n",
      "Then click  to open the Treatment Plan - Billing Diagnoses window and associate the visit diagnosis with each procedure.\n",
      "If this is the last or only visit in the treatment, click  Complete Treatment Plan when you've finished all the procedures in the plan.\n",
      "After Visit Summary, Analyst, App Orchard, ASAP, Beaker, BedTime, Bones, Break-the-Glass, Caboodle, Cadence, Canto, Care Everywhere, Charge Router, Chronicles, Clarity, Cogito ergo sum, Cohort, Colleague, Community Connect, Cupid, Epic, EpicCare, EpicCare Link, Epicenter, Epic Earth, EpicLink, EpicWeb, Good Better Best, Grand Central, Haiku, Happy Together, Healthy Planet, Hyperspace, Kaleidoscope, Kit, Limerick, Lucy, MyChart, OpTime, OutReach, Patients Like Mine, Phoenix, Powered by Epic, Prelude, Radar, Resolute, Revenue Guardian, Rover, Share Everywhere, SmartForms, Sonnet, Stork, Tapestry, Trove, Welcome, Willow, Wisdom, and With the Patient at Heart are registered trademarks, trademarks or service marks of Epic Systems Corporation in the United States of America and/or other countries.\n"
     ]
    }
   ],
   "source": [
    "# Extracting and summarizing\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 10\n",
    "parser = PlaintextParser.from_string(df1['Text'][26], Tokenizer(LANGUAGE))\n",
    "summarizer = LuhnSummarizer()\n",
    "summarizer = LuhnSummarizer(Stemmer(LANGUAGE))\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['LemmatizedText2'] = pd.DataFrame(df1.cleanText.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Label',\n",
       " 'Text',\n",
       " 'DocName',\n",
       " 'DocType',\n",
       " 'cleanText',\n",
       " 'Length',\n",
       " 'category_id',\n",
       " 'stemmedcleanText',\n",
       " 'LemmatizedcleanText',\n",
       " 'Text2',\n",
       " 'LemmatizedText2']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjudication match referral claim\n",
      "workqueues\n",
      "workqueue\n",
      "configuration\n",
      "configured\n",
      "configuring\n",
      "configure\n",
      "configurable\n",
      "useful\n",
      "matching\n",
      "batched\n",
      "field\n",
      "enter\n",
      "entering\n",
      "entered\n",
      "setting\n",
      "set\n",
      "extension use\n",
      "code\n",
      "following\n",
      "follow\n",
      "follows\n",
      "report\n",
      "reporting\n",
      "batch type\n",
      "evaluated matched\n",
      "rule\n",
      "service\n",
      "required\n",
      "require\n",
      "referred\n",
      "refer\n",
      "refers\n",
      "create\n",
      "created\n",
      "creates\n",
      "creating\n",
      "day\n",
      "date\n",
      "record\n",
      "work user\n",
      "cob\n",
      "table tapestry profile\n",
      "crossover\n",
      "secondary\n",
      "requirement benefit\n",
      "method\n",
      "topic\n",
      "allowed\n",
      "allow\n",
      "allows\n",
      "determine\n",
      "determination\n",
      "determines\n",
      "determining\n",
      "determined\n",
      "feature requires\n",
      "evaluate\n",
      "evaluates\n",
      "evaluation\n",
      "build\n",
      "building\n",
      "information automatic\n",
      "open\n",
      "coverage\n",
      "member\n",
      "select\n",
      "selection\n",
      "selecting\n",
      "organization\n",
      "screen\n",
      "need\n",
      "needed\n",
      "save\n",
      "saving\n",
      "saved\n",
      "specified\n",
      "specify\n",
      "specifying\n",
      "patient\n",
      "vendor\n",
      "contract\n",
      "contracting\n",
      "export\n",
      "exported\n",
      "exporting\n",
      "worked selected\n",
      "want\n",
      "schedule\n",
      "scheduler\n",
      "scheduled\n",
      "provider\n",
      "provides\n",
      "provide\n",
      "provided\n",
      "epic\n",
      "line\n",
      "adjudicate\n",
      "adjudicated\n",
      "adjudicating\n",
      "calculated\n",
      "calculation\n",
      "calculate\n",
      "calculates\n",
      "calculator\n",
      "calculating\n",
      "form\n",
      "item\n",
      "processing\n",
      "processed\n",
      "process\n",
      "working\n",
      "example\n",
      "menu\n",
      "payment\n",
      "file\n",
      "add\n",
      "automatically\n",
      "criterion\n",
      "copy\n"
     ]
    }
   ],
   "source": [
    "#ValueError: input must have more than one sentence. Hence have to use original Text with puntuations (periods) \n",
    "\n",
    "text=str(df1['LemmatizedcleanText'][0])\n",
    "# Summarize the text with ratio 0.01 (1% of total words.)\n",
    "#summarize(text,ratio=0.01)\n",
    "\n",
    "print(keywords(text,ratio=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n",
    "df1['stemmedText'] = pd.DataFrame(df1.Text.apply(lambda x: stemmed_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim\n",
      "claims\n",
      "thi\n",
      "use\n",
      "uses\n",
      "referr\n",
      "referral\n",
      "referrals\n",
      "workqueu\n",
      "workqueues\n",
      "workqueue\n",
      "field\n",
      "fields\n"
     ]
    }
   ],
   "source": [
    "text=str(df1['stemmedText'][0])\n",
    "print(keywords(text,ratio=0.01))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
