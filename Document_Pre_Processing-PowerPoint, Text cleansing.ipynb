{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-pptx\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/86/eb979f7b0333ec769041aae36df8b9f1bd8bea5bbad44620663890dce561/python-pptx-0.6.18.tar.gz (8.9MB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\l833377\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from python-pptx) (4.3.4)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in c:\\users\\l833377\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from python-pptx) (6.1.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in c:\\users\\l833377\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from python-pptx) (1.1.8)\n",
      "Building wheels for collected packages: python-pptx\n",
      "  Building wheel for python-pptx (setup.py): started\n",
      "  Building wheel for python-pptx (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\L833377\\AppData\\Local\\pip\\Cache\\wheels\\1f\\1f\\2c\\29acca422b420a0b5210bd2cd7e9669804520d602d2462f20b\n",
      "Successfully built python-pptx\n",
      "Installing collected packages: python-pptx\n",
      "Successfully installed python-pptx-0.6.18\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-pptx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction to CDW Module_August 2019.pptx\n",
      "----------------------\n",
      "Claims Data Warehouse\u000b",
      "Introduction to the CDW Module\n",
      "August 2019\n",
      "CDW@kp.org\n",
      "\n",
      "\n",
      "Welcome to the Claims Data Warehouse (CDW).  You can reach us at CDW@kp.org.\n",
      "\n",
      "In the Introduction to the CDW module, we will cover\n",
      "The CDW vision\n",
      "The CDW business approach\n",
      "An overview of the CDW data sources\n",
      "An overview of CDW highlights\n",
      "Introduction to the CDW Module\n",
      "CDW Vision\n",
      "CDW Vision\n",
      "Provide a trusted, consistent, timely, source agnostic, auditable source of data to enable business decision making.\n",
      "\n",
      "CDW is the national platform for finalized claims governed under the Health Plan Data Strategy program\n",
      "CDW initiative started in 2008 and is now a fully operational database\n",
      "CDW vision is to\n",
      "Provide a single trusted source of data that reflects the system of record\n",
      "Provide consistent and timely data availability\n",
      "Harmonize data across applications and different source systems\n",
      "Be SOX compliant with appropriate audit controls\n",
      "Support business decision making with high level of confidence\n",
      "Health Plan Data Strategy Business View\n",
      "The CDW has created a single, shared environment where every user will be able to access data for their reporting needs.\n",
      "Health Plan Data Strategy Business View\n",
      "The goal of the CDW is to create a ‘one-stop-shop’ for users data needs and limit the pull of data directly from the core transaction systems\n",
      "The CDW provides detailed atomic data from the source systems\n",
      "The CDW is partnered and operates in parallel with our Membership Data Warehouse – the MDW. The capability to link claims information and membership data is an enterprise objective.\n",
      "Health Plan Data Strategy Business View\n",
      "Value added attributes are also part of the CDW, for example groupers, utilization indicator, and allocated costs\n",
      "The CDW Governance process allows primary stakeholders an opportunity to participate and contribute to the CDW\n",
      "Health Plan Data Strategy Business View\n",
      "The CDW also provides standard reference data such as product and line of business, and HIPAA code descriptions\n",
      "\n",
      "Health Plan Data Strategy Business View\n",
      "The CDW provides users with data that will help to identify high frequency or high cost processes and opportunities for optimization.\n",
      "CDW Data Sources\n",
      "The CDW obtains data from a variety of sources, including TPA’s, KP ClaimsConnect, and other sources\n",
      "All source data is combined into a common data model, making the data transparent and available to users\n",
      "The CDW’s primary stakeholders, most of whom are represented on the CDW Governance, use the CDW data for various reporting needs\n",
      "CDW Highlights\n",
      "Adoption of the CDW continues to increase with every KPCC implementation.\n",
      "\n",
      "The CDW is under SOX controls for the following activities\n",
      "ETL’s run daily to load and balance data from all sources\n",
      "Daily system update SLA’s are targeted for 6:00 AM local time in the region\n",
      "Access and security per AALM requirements allows users to only access data that they have  received approval for\n",
      "\n",
      "The CDW is the primary data source for the following groups\n",
      "Pricing and the National Pricing Solution Application \n",
      "Actuarial Department\n",
      "Medicare Advantage Encounter Data Submission\n",
      "Regional Medicaid Risk Reporting\n",
      "ACA’s 2Rs and Cost Share Reduction – Membership and Claims Submissions\n",
      "State Insurance Commission Submissions – Medical Loss Ratios\n",
      "Data Methods tracking National Claims Quality and Auditing Activities\n",
      "Ad Hoc Financial and Performance Reporting\n",
      "Fraud, Waste, and Abuse Monitoring\n",
      "KPCC Reporting includes both Financial and Performance reporting\n",
      "Introduction to the CDW Module\n",
      "Thank you for viewing the Introduction to the CDW module.\n",
      "For more information about the CDW, visit the CDW Wiki at: https://wiki.kp.org/wiki/display/xdw/Claims+Data+Warehouse+-+Home\n",
      "If you have any questions about the information presented in this module, you may contact us at CDW@kp.org.\n",
      "Milliman (HCG and ICS).pptx\n",
      "----------------------\n",
      "The Milliman Healthcare Cost Guidelines (HCG) Grouper in CDW\n",
      "\n",
      "\n",
      "October, 2018\n",
      "\n",
      "Background\n",
      "In 2012, CDW implemented the Milliman HCG (Healthcare Cost Guidelines) grouper\n",
      "\n",
      "The Milliman grouper is a benchmarking tool that classifies medical services into one of 140+ categories. In addition, the tool also groups inpatient facility claims with the same admission date into episodes\n",
      "\n",
      "HCG was implemented upon request by National Pricing (NPS), as it is the foundation for their own service classification scheme, NPS Service Category\n",
      "\n",
      "Currently, the grouper is maintained by Decision Support. CDW leverages their implementation\n",
      "\n",
      "Examples of Categories\n",
      "HCG Assignment\n",
      "CDW runs data through the grouper using two separate processes,\n",
      "HCG and Inpatient Continuous Stay (ICS)\n",
      "\n",
      "HCG value assignment\n",
      "All new finalized claims as they arrive (on a daily or weekly basis)\n",
      "Not part of claims loads, so some delay in availability of HCG data\n",
      "\n",
      "HCG value assignment is atomic, each claim/service is treated in isolation\n",
      "Each claim gets a service category assigned\n",
      "Each service line gets a service category assigned\n",
      "We build negatives from the previous positive, we don’t send them to the grouper\n",
      "\n",
      "Using HCGs\n",
      "HCG data are modeled at the detail line level, with the claim level code repeated on each line\n",
      "Institutional\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Professional\n",
      "Using HCGs\n",
      "Join tables the “usual” way\n",
      "\n",
      "FROM CDW_V.INSTN_CLM_DTL DTL\n",
      "LEFT JOIN CDW_V.INSTN_CLM_DTL_HCG HCG\n",
      "\tON DTL.INSTN_CLM_IK=HCG.INSTN_CLM_IK\n",
      "\tAND DTL.INST_CLM_DTL_LNE_NB=HCG. INST_CLM_DTL_LNE_NB\n",
      "\tAND DTL.CLM_DTL_SBLNE_CD=HCG.CLM_DTL_SBLNE_CD\n",
      "\tAND DTL.RGN_CD=HCG.RGN_CD\n",
      "\n",
      "FROM CDW_V.PROF_CLM_DTL DTL\n",
      "LEFT JOIN CDW_V.PROF_CLM_DTL_HCG HCG\n",
      "\tON DTL.PROF_CLM_IK=HCG.PROF_CLM_IK\n",
      "\tAND DTL.PROF_CLM_DTL_LNE_NB=HCG. PROF_CLM_DTL_LNE_NB\n",
      "\tAND DTL.CLM_DTL_SBLNE_CD=HCG.CLM_DTL_SBLNE_CD\n",
      "\tAND DTL.RGN_CD=HCG.RGN_CD\n",
      "\n",
      "Using HCGs – Pharmacy Claims\n",
      "\n",
      "HCG assignments are also available for PBM Pharmacy claims. \n",
      "\n",
      "Join tables the “usual” way\n",
      "\n",
      "FROM CDW_V.PHRMCY_CLM C\n",
      "LEFT JOIN CDW_V.PHRMCY_CLM_HCG HCG\n",
      "\tON C.PHRMCY_CLM_IK=HCG.PHRMCY_CLM_IK\n",
      "\tAND C.RGN_CD=HCG.RGN_CD\n",
      "\n",
      "\n",
      "ICS\n",
      "Inpatient Continuous Stay (ICS):\n",
      "\n",
      "ICS grouping combines inpatient claims into an admission episode with a distinct ID (one of the claim numbers)\n",
      "\n",
      "CDW then creates a linkage table with CDW claim IKs joining the episode and its constituent claim versions, and identifies the previous episode ID for the admit, if any\n",
      "Only inpatient facility claims are included in the episode, not related professional claims\n",
      "The episode gets a single HCG service category assigned\n",
      "CDW exposes the admit count (one) and the grouper calculates the number of in patient days\n",
      "Readmits within 24 hours get added to the same episode\n",
      "\n",
      "Inpatient Continuous Stays\n",
      "\n",
      "ICS processing was created to solve for a historical issue in KP data. NPS was built assuming hospital stays were all encounters—i.e. internal services. “Claims” were built ONLY upon discharge. Whereas in the “real world”, extended and complex stays may result in multiple bills (supplemental bills, replacement bills, interim bills for stays spanning months). As a result, there was overcounting of admissions and days among external claims especially in the ROC regions\n",
      "\n",
      "\n",
      "Inpatient Continuous Stays\n",
      "\n",
      "Monthly, past and present inpatient institutional claims for the member any time a new paid in patient institutional claim or adjustment for the member/coverage (group) is received \n",
      "specifies a minimum admit date (rolling 36 month window)\n",
      "In patient status is determined by the bill type\n",
      "\n",
      "\n",
      "Using ICS\n",
      "ICS has two reporting tables:\n",
      "INPAT_CONT_STAY—the episode\n",
      "INSTN_CLM_INPAT_CONT_STAY_LNKG—a bridge between INPAT_CONT_STAY and the INSTN_CLM_HDR\n",
      "Using ICS\n",
      "Join INPAT_CONT_STAY to claims via INSTN_CLM_INPAT_STAY_LNKG\n",
      "CDWTBL_V.INPAT_CONT_STAY ICS\n",
      "LEFT JOIN CDWTBL_V. INSTN_CLM_INPAT_CONT_STAY_LNKG LNKG\n",
      "\tON ICS.INPAT_CONT_STAY_IK=LNKG.INPAT_CONT_STAY_IK\n",
      "\tAND ICS.RGN_CD=LNKG.RGN_CD\n",
      "LEFT JOIN CDWTBL_V.INSTN_CLM_HDR HDR\n",
      "\tON LNKG.INSTN_CLM_IK=HDR.INSTN_CLM_IK AND LNKG.RGN_CD=HDR.RGN_CD\n",
      "WHERE LNKG. MOST_RCNT_CONT_STAY_IN_CD=‘Y’\n",
      "Technicalities\n",
      "ETL generates rows in staging, in preparation for creation of a flat file for transmission to the grouper (Outbound or Input…outbound from CDW, input to the grouper)\n",
      "\n",
      "The grouper produces another flat file, which lands in another CDW staging table (Inbound or Output…inbound to CDW, output of the grouper)\n",
      "\n",
      "HCG can be run “any time”. It’s important that the ICS extract to staging occurs ASAP, since only the most recent version of the claim is sent, and claims can theoretically be adjusted the next day after finalizing\n",
      "\n",
      "\n",
      "Technicalities\n",
      "\n",
      "Descriptions and grouper version info is contained in: HCG_CD_REF\n",
      "\n",
      "The CDW_V view joins to HCG_CD_REF and exposes both the grouper version (2012, 2014, 2018, etc.) and the HCG code descriptions. The grouper version is NOT in the underlying table, it is derived in the view. There is a CR in flight (as of October 2018) to add the grouper version to CDWTBL_V\n",
      "\n",
      "Sources\n",
      "\n",
      "HCG is run for all active sources (i.e. no HCG for Diamond, QCare, CSI)\n",
      "\n",
      "No ICS for EMI (no institutional claims come from EMI)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for eachfile in glob.glob(\"*.pptx\"):\n",
    "    prs = Presentation(eachfile)\n",
    "    print(eachfile)\n",
    "    print(\"----------------------\")\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                print(shape.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"The brown fox wasn't that quick and he couldn't win the race\",\n",
    "          \"Hey that's a great deal! I just bought a phone for $199\",\n",
    "          \"@@You'll (learn) a **lot** in the book. Python is an amazing language!@@\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences] \n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\L833377\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The',\n",
      "   'brown',\n",
      "   'fox',\n",
      "   'was',\n",
      "   \"n't\",\n",
      "   'that',\n",
      "   'quick',\n",
      "   'and',\n",
      "   'he',\n",
      "   'could',\n",
      "   \"n't\",\n",
      "   'win',\n",
      "   'the',\n",
      "   'race']],\n",
      " [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'],\n",
      "  ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']],\n",
      " [['@',\n",
      "   '@',\n",
      "   'You',\n",
      "   \"'ll\",\n",
      "   '(',\n",
      "   'learn',\n",
      "   ')',\n",
      "   'a',\n",
      "   '**lot**',\n",
      "   'in',\n",
      "   'the',\n",
      "   'book',\n",
      "   '.'],\n",
      "  ['Python', 'is', 'an', 'amazing', 'language', '!'],\n",
      "  ['@', '@']]]\n"
     ]
    }
   ],
   "source": [
    "token_list = [tokenize_text(text) \n",
    "              for text in corpus]\n",
    "pprint(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<filter object at 0x0000022DF8ED6F28>, <filter object at 0x0000022DF8ED90F0>, <filter object at 0x0000022DF8ED9320>]\n"
     ]
    }
   ],
   "source": [
    "filtered_list_1 =  [filter(None,[remove_characters_after_tokenization(tokens) \n",
    "                                for tokens in sentence_tokens]) \n",
    "                    for sentence_tokens in token_list]\n",
    "print(filtered_list_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters_before_tokenization(sentence,\n",
    "                                          keep_apostrophes=False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]'\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    else:\n",
    "        PATTERN = r'[^a-zA-Z0-9 ]'\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The brown fox wasnt that quick and he couldnt win the race', 'Hey thats a great deal I just bought a phone for 199', 'Youll learn a lot in the book Python is an amazing language']\n"
     ]
    }
   ],
   "source": [
    "filtered_list_2 = [remove_characters_before_tokenization(sentence) \n",
    "                    for sentence in corpus]    \n",
    "print(filtered_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal! I just bought a phone for 199\", \"You'll learn a lot in the book. Python is an amazing language!\"]\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus = [remove_characters_before_tokenization(sentence, keep_apostrophes=True) \n",
    "                  for sentence in corpus]\n",
    "print(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(sentence, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "    return expanded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The brown fox was not that quick and he could not win the race', 'Hey that is a great deal! I just bought a phone for 199', 'You will learn a lot in the book. Python is an amazing language!']\n"
     ]
    }
   ],
   "source": [
    "expanded_corpus = [expand_contractions(sentence, contraction_mapping) \n",
    "                    for sentence in cleaned_corpus]    \n",
    "print(expanded_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the brown fox wasn't that quick and he couldn't win the race\n",
      "THE BROWN FOX WASN'T THAT QUICK AND HE COULDN'T WIN THE RACE\n"
     ]
    }
   ],
   "source": [
    "# case conversion    \n",
    "print(corpus[0].lower())\n",
    "print(corpus[0].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LookupError: \n",
    "**********************************************************************\n",
    "  Resource stopwords not found.\n",
    "  Please use the NLTK Downloader to obtain the resource:\n",
    "\n",
    "  >>> import nltk\n",
    "  >>> nltk.download('stopwords')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\L833377\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The', 'brown', 'fox', 'quick', 'could', 'win', 'race']], [['Hey', 'great', 'deal', '!'], ['I', 'bought', 'phone', '199']], [['You', 'learn', 'lot', 'book', '.'], ['Python', 'amazing', 'language', '!']]]\n"
     ]
    }
   ],
   "source": [
    "# removing stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    return filtered_tokens\n",
    "    \n",
    "expanded_corpus_tokens = [tokenize_text(text)\n",
    "                          for text in expanded_corpus]    \n",
    "filtered_list_3 =  [[remove_stopwords(tokens) \n",
    "                        for tokens in sentence_tokens] \n",
    "                        for sentence_tokens in expanded_corpus_tokens]\n",
    "print(filtered_list_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing repeated characters\n",
    "sample_sentence = 'My schooool is realllllyyy amaaazingggg'\n",
    "sample_sentence_tokens = tokenize_text(sample_sentence)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LookupError: \n",
    "**********************************************************************\n",
    "  Resource wordnet not found.\n",
    "  Please use the NLTK Downloader to obtain the resource:\n",
    "\n",
    "  >>> import nltk\n",
    "  >>> nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\L833377\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'school', 'is', 'really', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "print(remove_repeated_characters(sample_sentence_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porter stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "lie\n",
      "strang\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped'))\n",
    "print(ps.stem('lying'))\n",
    "print(ps.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lancaster stemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "lying\n",
      "strange\n"
     ]
    }
   ],
   "source": [
    "print(ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped'))\n",
    "\n",
    "print(ls.stem('lying'))\n",
    "\n",
    "print(ls.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex stemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing$|s$|ed$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "ly\n",
      "strange\n"
     ]
    }
   ],
   "source": [
    "print(rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped'))\n",
    "\n",
    "print(rs.stem('lying'))\n",
    "\n",
    "print(rs.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snowball stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported Languages: ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "# can come in handy if the text has words from foreign language listed below\n",
    "print('Supported Languages:', SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autobahn'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autobahnen -> cars\n",
    "# autobahn -> car\n",
    "ss.stem('autobahnen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spring'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# springen -> jumping\n",
    "# spring -> jump\n",
    "ss.stem('springen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization is similar to stemming where word affixes are removed to get to a base form. However here base form is also known as the root word (not the root stem) which will always be present in the dictionary. Hence this process takes longer as additional step pf looking for the root word in the dictionary is involved. The nltk package has a robust lemmatization modulethat uses WordNet and the word's syntax and semantics.  \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "men\n",
      "run\n",
      "eat\n",
      "sad\n",
      "fancy\n",
      "ate\n",
      "fancier\n"
     ]
    }
   ],
   "source": [
    "# lemmatize nouns\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('men', 'n'))\n",
    "\n",
    "# lemmatize verbs\n",
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('ate', 'v'))\n",
    "\n",
    "# lemmatize adjectives\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))\n",
    "\n",
    "# ineffective lemmatization\n",
    "print(wnl.lemmatize('ate', 'n'))\n",
    "print(wnl.lemmatize('fancier', 'v'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
